
# coding: utf-8

# # Imports
import re
import csv
import math
import collections
# In[29]:

def erase_link(tweet):
    inx = tweet.find('http')
    #Check to see if tweet contains a link
    if inx != -1:
        #Find white space
        sp_inx = tweet.find(" ", inx)
        #Check if there is a white space after the link
        if sp_inx != -1:
            new_tweet = tweet[0:inx] + tweet[sp_inx + 1:]
        else:
            new_tweet = tweet[0:inx]
        return new_tweet
    return tweet

# In[30]:

#This function will normalize a 
#tweet passed in as a string. 
#The output will be the text with out
#any special characters in all lowercase.
def normalize(tweet):
    #erase link
    normalizedTweet = erase_link(tweet)
    #print normalizedTweet
    normalizedTweet = re.compile('[^a-zA-Z@]').sub(' ', normalizedTweet)
    normalizedTweet = normalizedTweet.lower()
    normalizedTweet = " ".join(normalizedTweet.split())
    return normalizedTweet

#with open("twitter-data/testdata.manual.2009.06.14.csv", 'rb') as csvfile:
with open("twitter-data/testdata2.csv", 'rb') as csvfile:
    r = csv.reader(csvfile, delimiter=',', quotechar='"')
    docs = [ (x[2], x[4], x[5])  for x in r]
    
tweet_date = [ d[0] for d in docs ] #Tweet dates
tweet_user = [ d[1] for d in docs ] #Tweet users
tweet_text = [ d[2] for d in docs ] #Tweet text

#Normalize each tweet 
tweet_text = map(normalize, tweet_text)


# In[32]:

corpus = {}
for x in range(0,len(tweet_text)):
    corpus[x] = [tweet_date[x], tweet_user[x],tweet_text[x]]
    #print corpus[x]



# # Inverted Indexs
# 
# For the twitter data, there will be two inverted indexes. The first inverted index will have the username as the key and the tweets that correspond to that username as the values. The second inverted index will be the word index for our tweets; the key will be a word and the values will be the id of the documents that contain that word.

# # Word Dictionary Reversed Index
# 
# The reversed index for the words in the corpus will be created by going through each word in each document. The reveresed index will allow for a faster retrieval of relevant documents. 

# In[33]:

def create_inverted_index(corpus):
    idx = {}
    
    for i, doc in enumerate(corpus):
        # Iterate through each word in the document
        for word in doc.split():
            if word in idx:
                # Update the document's term frequency
                if i in idx[word]:
                    idx[word][i] += 1
                # Add the document to the word index
                else:
                    idx[word][i] = 1;
            # Add the word to the reversed index
            else:
                idx[word] = {i:1}
    
    return idx


# In[34]:

'''
test_users = ["vcu451", "chadfu", "SIX15"]
test_corpus = ["reading my kindle2  love it lee childs is good read", 
               "ok, first assesment of the kindle2 ...it fucking rocks", 
               "fuck this economy I hate aig and their non loan given asses"]
'''

idx = create_inverted_index(tweet_text)


# # User Tweet Index
# 
# The user tweet index will allow for the all tweets(documents) that belong to a username to be retrieved quickly.

# In[35]:

def create_user_index(users):
    
    user_tweets = {}
    
    # Go through each of the tweets in the corpus
    for i in range( len(users) ):
        # When user already exists, add the document id to the existing user tweet list
        if users[i] in user_tweets:
            user_tweets[users[i] ].append(i)
        # Otherwise, creat a new list with the document id
        else:
            user_tweets[users[i] ] = [i]
            
    return user_tweets


# In[36]:

#user_tweet_index = create_user_index(tweet_user)

#user_tweet_index


# # Document Ranking
# To rank the documents, we will be implementing two different ranking algorithms: TF-IDF and BM25. 

# # TF-IDF
# The TF-IDF (term frequencyâ€“inverse document frequency) ranking algorithm ranks documents based on the term frequency of the words in the query in relation to the words in the documents.

# In[37]:

def print_results(results, n, head=True):
    ''' Helper function to print results
    '''
    if head:
        print('\nTop %d from recall set of %d items:' % (n, len(results) ) )
        for r in results[:n]:
            print('\t%0.2f - %s' % (r[0], tweet_text[r[1]]))
    else:
        print('\nTop %d from recall set of %d items:' % (n, len(results) ) )
        for r in results[:n]:
            print('\t%0.2f - %s' % (r[0], tweet_text[r[1]]))

# # BM25
# Implement the BM25 ranking algorithm to rank our results. This ranking algorithm is the one we will be using for the final version of our search engine.

# In[40]:

def get_results_bm25(qry, corpus, k1=1.5, b=0.75):
    idx = create_inverted_index(corpus)
    
    # n - the length of the corpus
    n = len(corpus)
    
    # d - list with elements corresponding to the length of each document
    d = [len(x.split()) for x in corpus]
    
    # d_avg - the average document length of the docuemnts in the corpus
    d_avg = float(sum(d) / len(d))
    score = collections.Counter()
    for term in qry.split():
        if term in idx:
            i = idf(term, idx, n)
            for doc in idx[term]:
                # f - the number of times the term appears in the document
                f = float(idx[term][doc])
                # s - the BM25 score for this (term, docuemnt) pair
                s = i * ( (f * (k1 + 1) ) / (f + k1 * (1 - b + (b * (float(d[doc] ) / d_avg) ) ) ) )
                score[doc] += s
                
    results = []
    for x in [ [r[0], r[1] ] for r in zip(score.keys(), score.values() )]:
        if x[1] > 0:
            results.append([ x[1], x[0] ])
            
    sorted_results = sorted(results, key=lambda t: t[0] * -1)
    return sorted_results

#results = get_results_bm25('hate', tweet_text, k1=1.5, b=0.75)

def search_all_terms(corpus):
    idx = create_inverted_index(corpus)
    
    print len(idx)

print ("Starting Search On All Terms")

search_all_terms(tweet_text);

print ("Completed Searching All Terms")

