{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import math\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def erase_link(tweet):\n",
    "    inx = tweet.find('http')\n",
    "    #Check to see if tweet contains a link\n",
    "    if inx != -1:\n",
    "        #Find white space\n",
    "        sp_inx = tweet.find(\" \", inx)\n",
    "        #Check if there is a white space after the link\n",
    "        if sp_inx != -1:\n",
    "            new_tweet = tweet[0:inx] + tweet[sp_inx + 1:]\n",
    "        else:\n",
    "            new_tweet = tweet[0:inx]\n",
    "        return new_tweet\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myapple @rocks yes\n"
     ]
    }
   ],
   "source": [
    "#This function will normalize a \n",
    "#tweet passed in as a string. \n",
    "#The output will be the text with out\n",
    "#any special characters in all lowercase.\n",
    "def normalize(tweet):\n",
    "    #erase link\n",
    "    normalizedTweet = erase_link(tweet)\n",
    "    #print normalizedTweet\n",
    "    normalizedTweet = re.compile('[^a-zA-Z@]').sub(' ', normalizedTweet)\n",
    "    normalizedTweet = normalizedTweet.lower()\n",
    "    normalizedTweet = \" \".join(normalizedTweet.split())\n",
    "    return normalizedTweet\n",
    "\n",
    "print normalize(\"123myAPPLE!@Rocks http.bit.ly YES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"twitter-data/testdata.manual.2009.06.14.csv\", 'rb') as csvfile:\n",
    "    r = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    docs = [ (x[2], x[4], x[5])  for x in r]\n",
    "    \n",
    "tweet_date = [ d[0] for d in docs ] #Tweet dates\n",
    "tweet_user = [ d[1] for d in docs ] #Tweet users\n",
    "tweet_text = [ d[2] for d in docs ] #Tweet text\n",
    "\n",
    "#Normalize each tweet \n",
    "tweet_text = map(normalize, tweet_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "for x in range(0,len(tweet_text)):\n",
    "    corpus[x] = [tweet_date[x], tweet_user[x],tweet_text[x]]\n",
    "    #print corpus[x]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Inverted Indexs\n",
    "\n",
    "For the twitter data, there will be two inverted indexes. The first inverted index will have the username as the key and the tweets that correspond to that username as the values. The second inverted index will be the word index for our tweets; the key will be a word and the values will be the id of the documents that contain that word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Dictionary Reversed Index\n",
    "\n",
    "The reversed index for the words in the corpus will be created by going through each word in each document. The reveresed index will allow for a faster retrieval of relevant documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_inverted_index(corpus):\n",
    "    idx = {}\n",
    "    \n",
    "    for i, doc in enumerate(corpus):\n",
    "        # Iterate through each word in the document\n",
    "        for word in doc.split():\n",
    "            if word in idx:\n",
    "                # Update the document's term frequency\n",
    "                if i in idx[word]:\n",
    "                    idx[word][i] += 1\n",
    "                # Add the document to the word index\n",
    "                else:\n",
    "                    idx[word][i] = 1;\n",
    "            # Add the word to the reversed index\n",
    "            else:\n",
    "                idx[word] = {i:1}\n",
    "    \n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "test_users = [\"vcu451\", \"chadfu\", \"SIX15\"]\n",
    "test_corpus = [\"reading my kindle2  love it lee childs is good read\", \n",
    "               \"ok, first assesment of the kindle2 ...it fucking rocks\", \n",
    "               \"fuck this economy I hate aig and their non loan given asses\"]\n",
    "'''\n",
    "\n",
    "idx = create_inverted_index(tweet_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Tweet Index\n",
    "\n",
    "The user tweet index will allow for the all tweets(documents) that belong to a username to be retrieved quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_user_index(users):\n",
    "    \n",
    "    user_tweets = {}\n",
    "    \n",
    "    # Go through each of the tweets in the corpus\n",
    "    for i in range( len(users) ):\n",
    "        # When user already exists, add the document id to the existing user tweet list\n",
    "        if users[i] in user_tweets:\n",
    "            user_tweets[users[i] ].append(i)\n",
    "        # Otherwise, creat a new list with the document id\n",
    "        else:\n",
    "            user_tweets[users[i] ] = [i]\n",
    "            \n",
    "    return user_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_tweet_index = create_user_index(tweet_user)\n",
    "\n",
    "#user_tweet_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Document Ranking\n",
    "To rank the documents, we will be implementing two different ranking algorithms: TF-IDF and BM25. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "The TF-IDF (term frequencyâ€“inverse document frequency) ranking algorithm ranks documents based on the term frequency of the words in the query in relation to the words in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_results(results, n, head=True):\n",
    "    ''' Helper function to print results\n",
    "    '''\n",
    "    if head:\n",
    "        print('\\nTop %d from recall set of %d items:' % (n, len(results) ) )\n",
    "        for r in results[:n]:\n",
    "            print('\\t%0.2f - %s' % (r[0], tweet_text[r[1]]))\n",
    "    else:\n",
    "        print('\\nTop %d from recall set of %d items:' % (n, len(results) ) )\n",
    "        for r in results[:n]:\n",
    "            print('\\t%0.2f - %s' % (r[0], tweet_text[r[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.81270480423\n",
      "3.32022831913\n",
      "4.60116216459\n",
      "5.51745289646\n",
      "5.51745289646\n"
     ]
    }
   ],
   "source": [
    "def idf(term, idx, n):\n",
    "    # term - the term that is being scored\n",
    "    # idx - the reversed index on the terms in the corpus\n",
    "    # n - the number of docments the term appears in\n",
    "    return math.log(float(n) / (1 + len(idx[term])))\n",
    "    \n",
    "print(idf('how', idx, len(tweet_user)))\n",
    "print(idf('hate', idx, len(tweet_user)))\n",
    "print(idf('sleep', idx, len(tweet_user)))\n",
    "print(idf('whoopi', idx, len(tweet_user)))\n",
    "print(idf('monkeys', idx, len(tweet_user)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 from recall set of 17 items:\n",
      "\t13.28 - on that note i hate word i hate pages i hate latex there i said it i hate latex all you texn rds can come kill me now\n",
      "\t3.32 - i hate revision it s so boring i am totally unprepared for my exam tomorrow things are not looking good\n",
      "\t3.32 - my wrist still hurts i have to get it looked at i hate the dr dentist scary places time to watch eagle eye if you want to join txt\n",
      "\t3.32 - hate safeway select green tea icecream bought two cartons what a waste of money gt lt\n",
      "\t3.32 - fuck this economy i hate aig and their non loan given asses\n",
      "\t3.32 - i hate comcast right now everything is down cable internet amp phone ughh what am i to do\n",
      "\t3.32 - i srsly hate the stupid twitter api timeout thing soooo annoying\n",
      "\t3.32 - son has me looking at cars online i hate car shopping would rather go to the dentist anyone with a good car at a good price to sell\n",
      "\t3.32 - shaunwoo hate n on aig\n",
      "\t3.32 - nooooooo my dvr just died and i was only half way through the ea presser hate you time warner\n"
     ]
    }
   ],
   "source": [
    "def get_results_tfidf(qry, idx, n):\n",
    "    score = collections.Counter()\n",
    "    for term in qry.split():\n",
    "        if term in idx:\n",
    "            i = idf(term, idx, n)\n",
    "            for doc in idx[term]:\n",
    "                score[doc] += idx[term][doc] * i\n",
    "    results=[]\n",
    "    for x in [[r[0],r[1]] for r in zip(score.keys(), score.values())]:\n",
    "        if x[1] > 0:\n",
    "            results.append([x[1],x[0]])\n",
    "    sorted_results= sorted(results, key=lambda t : t[0] * -1)\n",
    "    return sorted_results\n",
    "#results = get_results_tfidf('monkeys', idx, len(tweet_user))\n",
    "#results = get_results_tfidf('hate', idx, len(tweet_user))\n",
    "#results = get_results_tfidf('sleep', idx, len(tweet_user))\n",
    "results = get_results_tfidf('hate', idx, len(tweet_user))\n",
    "\n",
    "print_results(results, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25\n",
    "Implement the BM25 ranking algorithm to rank our results. This ranking algorithm is the one we will be using for the final version of our search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_results_bm25(qry, corpus, k1=1.5, b=0.75):\n",
    "    idx = create_inverted_index(corpus)\n",
    "    \n",
    "    # n - the length of the corpus\n",
    "    n = len(corpus)\n",
    "    \n",
    "    # d - list with elements corresponding to the length of each document\n",
    "    d = [len(x.split()) for x in corpus]\n",
    "    \n",
    "    # d_avg - the average document length of the docuemnts in the corpus\n",
    "    d_avg = float(sum(d) / len(d))\n",
    "    score = collections.Counter()\n",
    "    for term in qry.split():\n",
    "        if term in idx:\n",
    "            i = idf(term, idx, n)\n",
    "            for doc in idx[term]:\n",
    "                # f - the number of times the term appears in the document\n",
    "                f = float(idx[term][doc])\n",
    "                # s - the BM25 score for this (term, docuemnt) pair\n",
    "                s = i * ( (f * (k1 + 1) ) / (f + k1 * (1 - b + (b * (float(d[doc] ) / d_avg) ) ) ) )\n",
    "                score[doc] += s\n",
    "                \n",
    "    results = []\n",
    "    for x in [ [r[0], r[1] ] for r in zip(score.keys(), score.values() )]:\n",
    "        if x[1] > 0:\n",
    "            results.append([ x[1], x[0] ])\n",
    "            \n",
    "    sorted_results = sorted(results, key=lambda t: t[0] * -1)\n",
    "    return sorted_results\n",
    "\n",
    "results = get_results_bm25('hate', tweet_text, k1=1.5, b=0.75)\n",
    "#print_results(results, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Get content based on username\n",
    "This function will retrieve tweets in the corpus that belong to the specified user. These tweets will be ordered by date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 02 04:29:16 UTC 2009 SimpleManJess ncaa baseball super regional rams club\n",
      "Tue Jun 02 04:29:17 UTC 2009 SimpleManJess baseballamerica com blog baseball america prospects blog blog\n"
     ]
    }
   ],
   "source": [
    "def print_results_username(username_index, user_name):\n",
    "    if(user_name in username_index):\n",
    "        # All of the documents that correspond to the username \n",
    "        docs = username_index[user_name]\n",
    "        # For every doc, print date, username, and tweet \n",
    "        for doc in docs:\n",
    "            print tweet_date[doc] + \" \" + tweet_user[doc] + \" \" + tweet_text[doc]\n",
    "    else:\n",
    "        print \"Username does not exist\"\n",
    "    \n",
    "results = create_user_index(tweet_user)\n",
    "print_results_username(results, 'SimpleManJess')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine User Query\n",
    "This will take in the user query and determine whether or not the user is wishin to search all tweet content or a specific user's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def determine_query(query):\n",
    "    if(query[0] == '@' and len(query.split()) == 1):\n",
    "        user_choice = raw_input((\"Select your query preference\",\n",
    "                                \"\\n1. All content\",\n",
    "                                \"\\n2. User content\"))\n",
    "        return user_choice\n",
    "    return '1'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def u_search(user_choice, query):\n",
    "    if(user_choice == '1'):\n",
    "        results = get_results_bm25(query, tweet_text)\n",
    "        print_results(results,25)\n",
    "    if(user_choice == '2'):\n",
    "        results = create_user_index(tweet_user)\n",
    "        print_results_username(results, query)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 25 from recall set of 3 items:\n",
      "\t7.38 - @kirstiealley pet dentist\n",
      "\t6.09 - @kirstiealley i hate going to the dentist\n",
      "\t5.60 - @kirstiealley my dentist is great but she s expensive\n"
     ]
    }
   ],
   "source": [
    "query = normalize(\"@kirstiealley\")\n",
    "#choice = determine_query(query)\n",
    "u_search('1',query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
